{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación de una Red Neuronal utilizando Pytorch para Fashion-MNIST"
      ],
      "metadata": {
        "id": "5ZPl8tb7i6Vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTGjYlAZhHt1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJVCvDL5hM0i",
        "outputId": "2fbbbbed-f2df-4970-d00e-b866d624be01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/datasets/fashion-mnist_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/datasets/fashion-mnist_test.csv')"
      ],
      "metadata": {
        "id": "FWBpQ2-uhPGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reshape de las Imágenes\n",
        "Cuando cargamos imágenes desde un CSV, cada imagen está representada como una fila de números en el CSV, que son los valores de los píxeles. Para un dataset como Fashion-MNIST, cada imagen tiene un tamaño de 28x28 píxeles. Sin embargo, al cargarlas desde el CSV, cada imagen se convierte en un array de 784 elementos (28 multiplicado por 28). Por lo tanto, necesitamos reshape estos arrays para transformarlos de nuevo en la estructura de imagen 2D que representa su formato original.\n",
        "\n",
        "Esto es importante porque las operaciones que realizamos en imágenes (como visualización y convoluciones en redes neuronales) esperan datos en forma de matriz o en un formato de varias dimensiones (altura, ancho, canales). Al hacer reshape(-1, 28, 28, 1), estamos configurando los datos para tener la siguiente forma:\n",
        "\n",
        "-1: Infiera la dimensión de este eje basándose en la longitud de los datos y las otras dimensiones especificadas. En práctica, esto se traduce en el número de imágenes.\n",
        "28, 28: Estas son las dimensiones de cada imagen (altura x ancho).\n",
        "1: Esto indica el número de canales de color; en este caso, 1, porque las imágenes son en escala de grises."
      ],
      "metadata": {
        "id": "VZ_bwa9ymku6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar etiquetas y imágenes\n",
        "train_labels = train_data['label'].values\n",
        "train_images = train_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
        "test_labels = test_data['label'].values\n",
        "test_images = test_data.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0"
      ],
      "metadata": {
        "id": "xvz4gxo3l1v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomFashionMNISTDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "6sh7Aq8Sl85L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Transformaciones\n",
        "Las transformaciones se utilizan para realizar operaciones de preprocesamiento y aumento de datos en las imágenes. Esto puede incluir normalización, recorte, rotación, y otras formas de alteración de la imagen que ayudan a generalizar y mejorar el entrenamiento de modelos de machine learning.\n",
        "\n",
        "En el código que hemos definido, usamos una serie de transformaciones:\n",
        "\n",
        "ToPILImage(): Convierte un array de numpy o un tensor de PyTorch en una imagen PIL. Esto es necesario porque muchas de las transformaciones en torchvision esperan imágenes en formato PIL.\n",
        "ToTensor(): Convierte la imagen PIL (o numpy.ndarray) en un tensor de PyTorch. Esto también cambia el rango de los píxeles de 0-255 a 0-1, reescalandolos mediante una división por 255.\n",
        "Normalize((0.5,), (0.5,)): Normaliza un tensor de imagen con una media y una desviación estándar. En este caso, la normalización (0.5, 0.5) transforma los píxeles a un rango de [-1, 1], facilitando la convergencia durante el entrenamiento al centrar los datos.\n",
        "Estas transformaciones son cruciales para preparar los datos de manera que sean más efectivos para entrenar redes neuronales, ayudando a que el modelo aprenda más eficientemente."
      ],
      "metadata": {
        "id": "sV-iJ1afmru0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformaciones para las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convertir numpy arrays a PILImage para transformar\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Crear datasets\n",
        "train_dataset = CustomFashionMNISTDataset(train_images, train_labels, transform=transform)\n",
        "test_dataset = CustomFashionMNISTDataset(test_images, test_labels, transform=transform)\n",
        "\n",
        "# Creando los data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "2SZwc9HWmLhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Definición de la red neuronal"
      ],
      "metadata": {
        "id": "TmTM5xDsnC5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)  # Capa de entrada (28*28 = 784)\n",
        "        self.fc2 = nn.Linear(256, 128)  # Primera capa oculta\n",
        "        self.fc3 = nn.Linear(128, 64)   # Segunda capa oculta\n",
        "        self.fc4 = nn.Linear(64, 10)    # Capa de salida (10 clases)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)  # Aplanar la imagen\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "te88pbvgnIlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Instanciar el modelo, el criterio de pérdida y el optimizador\n"
      ],
      "metadata": {
        "id": "y3H48HSfnLss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FashionMNISTModel()\n",
        "criterion = nn.CrossEntropyLoss()  # Usado comúnmente para clasificación multiclase\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)  # Optimizador Adam\n"
      ],
      "metadata": {
        "id": "_7eH-dMsnNzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Entrenar la red\n"
      ],
      "metadata": {
        "id": "bSm_AjLunMv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # Número de épocas para entrenar\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()           # Limpia los gradientes\n",
        "        output = model(images)          # Pasa las imágenes por la red\n",
        "        loss = criterion(output, labels)  # Calcula la pérdida\n",
        "        loss.backward()                 # Propaga la pérdida hacia atrás\n",
        "        optimizer.step()                # Actualiza los parámetros\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9D-29XjnPyx",
        "outputId": "aa9ce903-35da-4307-ff53-9e5243ee4f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training loss: 0.5233497702395484\n",
            "Epoch 2 - Training loss: 0.39506281205395394\n",
            "Epoch 3 - Training loss: 0.3645390493751589\n",
            "Epoch 4 - Training loss: 0.3363837057958915\n",
            "Epoch 5 - Training loss: 0.3187387602955802\n",
            "Epoch 6 - Training loss: 0.30826748512002194\n",
            "Epoch 7 - Training loss: 0.2951043083358294\n",
            "Epoch 8 - Training loss: 0.2870108994053625\n",
            "Epoch 9 - Training loss: 0.27647093009910606\n",
            "Epoch 10 - Training loss: 0.2688553388685242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluación del modelo"
      ],
      "metadata": {
        "id": "zOfDGDwBnTU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test set: {accuracy}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYRX3hganTAJ",
        "outputId": "200be499-0b5c-492d-f72f-d9fcf0a95905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 88.82%\n"
          ]
        }
      ]
    }
  ]
}